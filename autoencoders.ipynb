{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1KDVMFp2huB",
        "outputId": "2163dc6f-da87-4a72-b2d9-b216d6205abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1+cpu\n",
            "GPU available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXkXPLJ3lJp"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUE38YH82jd9",
        "outputId": "341171a3-3980-43fa-b59b-6a63a700e403"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install piq\n",
        "!pip install torchmetrics\n",
        "from piq import ssim, ms_ssim, psnr\n",
        "from torchmetrics import (\n",
        "    PeakSignalNoiseRatio,\n",
        "    StructuralSimilarityIndexMeasure,\n",
        "    MeanSquaredError,\n",
        "    MultiScaleStructuralSimilarityIndexMeasure,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvpQRcVa4tRN"
      },
      "source": [
        "Kodak Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Dr6Gklm4vOk",
        "outputId": "1f7bd15d-ef5c-47b5-ad97-63855507a219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading kodim01.png...\n",
            "Downloading kodim02.png...\n",
            "Downloading kodim03.png...\n",
            "Downloading kodim04.png...\n",
            "Downloading kodim05.png...\n",
            "Downloading kodim06.png...\n",
            "Downloading kodim07.png...\n",
            "Downloading kodim08.png...\n",
            "Downloading kodim09.png...\n",
            "Downloading kodim10.png...\n",
            "Downloading kodim11.png...\n",
            "Downloading kodim12.png...\n",
            "Downloading kodim13.png...\n",
            "Downloading kodim14.png...\n",
            "Downloading kodim15.png...\n",
            "Downloading kodim16.png...\n",
            "Downloading kodim17.png...\n",
            "Downloading kodim18.png...\n",
            "Downloading kodim19.png...\n",
            "Downloading kodim20.png...\n",
            "Downloading kodim21.png...\n",
            "Downloading kodim22.png...\n",
            "Downloading kodim23.png...\n",
            "Downloading kodim24.png...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Kodak datasetini indir\n",
        "def download_kodak_dataset(dest_dir='./kodak'):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    for i in range(1, 25):\n",
        "        filename = f'kodim{str(i).zfill(2)}.png'\n",
        "        url = f\"http://r0k.us/graphics/kodak/kodak/{filename}\"\n",
        "        dest = os.path.join(dest_dir, filename)\n",
        "        if not os.path.exists(dest):\n",
        "            print(f\"Downloading {filename}...\")\n",
        "            urllib.request.urlretrieve(url, dest)\n",
        "        else:\n",
        "            print(f\"{filename} already exists. Skipping download.\")\n",
        "\n",
        "# Kodak Dataset sınıfı\n",
        "class KodakDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        self.img_paths = [os.path.join(img_dir, f) for f in sorted(os.listdir(img_dir))\n",
        "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, 0\n",
        "    \n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "download_kodak_dataset('./kodak')\n",
        "kodak_dataset = KodakDataset('./kodak', transform=transform)\n",
        "kodak_dataset1 = KodakDataset('./kodak', transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69r62uSiwzOU"
      },
      "source": [
        "DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg_ksOGOwzIg",
        "outputId": "e9a97b1c-6cfe-41ad-a845-3f3e13c7f7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 256, 256])\n",
            "torch.Size([4, 3, 256, 256])\n",
            "torch.Size([4, 3, 256, 256])\n",
            "torch.Size([4, 3, 256, 256])\n",
            "torch.Size([4, 3, 256, 256])\n",
            "torch.Size([4, 3, 256, 256])\n",
            "Kodak datasetinde 24 resim var.\n",
            "DataLoader batch sayısı: 6\n",
            "DataLoader batch sayısı: 1\n",
            "Kodak size: 24\n"
          ]
        }
      ],
      "source": [
        "\n",
        "kodak_loader = torch.utils.data.DataLoader(kodak_dataset, batch_size=4, shuffle=True)\n",
        "kodak_loader1 = torch.utils.data.DataLoader(kodak_dataset1, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "for img, label in kodak_loader:\n",
        "  print(img.shape)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Kodak datasetinde {len(kodak_dataset)} resim var.\")\n",
        "print(f\"DataLoader batch sayısı: {len(kodak_loader)}\")\n",
        "print(f\"DataLoader batch sayısı: {len(kodak_loader1)}\")\n",
        "\n",
        "print(\"Kodak size:\", len(kodak_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poC8IeMLD0Ge"
      },
      "source": [
        "Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C6pKoF98OKh",
        "outputId": "47bc3686-ec21-4792-c9b2-77ad25019199"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "mse_metric = MeanSquaredError().to(device)\n",
        "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
        "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "ms_ssim_metric = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "\n",
        "def mse_loss(output, target):\n",
        "    return mse_metric(output, target)\n",
        "\n",
        "def psnr_loss(output, target):\n",
        "    psnr_val = psnr_metric(output, target)\n",
        "    psnr_norm = psnr_val / 50.0\n",
        "    return 1 - psnr_norm\n",
        "\n",
        "def ssim_loss(output, target):\n",
        "    return 1 - ssim_metric(output, target)\n",
        "\n",
        "\n",
        "def combined_loss(output, target, alpha=0.5):\n",
        "    # alpha ile MSE ve 1-SSIM ağırlıklandırılıyor\n",
        "    mse = mse_loss(output, target)\n",
        "    ssim_val = ssim(output, target)\n",
        "    # Not: SSIM yüksekse iyi, ama loss için 1-SSIM kullanılır\n",
        "    return alpha * mse + (1 - alpha) * (1 - ssim_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLfTej5Vty9J"
      },
      "source": [
        "Compression Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EuZQt_Fqty1W"
      },
      "outputs": [],
      "source": [
        "def compression_ratio(original_img, latent_tensor, quantization_levels=16):\n",
        "    B, C, H, W = original_img.shape\n",
        "    original_size_bits = B * C * H * W * 32  # float32 = 32 bit\n",
        "\n",
        "    B_l, C_l, H_l, W_l = latent_tensor.shape\n",
        "    bits_per_value = int(torch.log2(torch.tensor(quantization_levels)).item())  # örn. 4 bit\n",
        "    compressed_size_bits = B_l * C_l * H_l * W_l * bits_per_value\n",
        "\n",
        "    return original_size_bits / compressed_size_bits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC3Wotm48bWM"
      },
      "source": [
        "Basic Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MOP5Db6a8bE2"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        reconstructed = self.decoder(latent)\n",
        "        return reconstructed, latent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Gn9Bvw7J8B"
      },
      "source": [
        "Latent Quantizer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mSf-xQND7Jcy"
      },
      "outputs": [],
      "source": [
        "def quantize(tensor, levels=16):\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "    scale = (max_val - min_val) / (levels - 1)\n",
        "    return torch.round((tensor - min_val) / scale) * scale + min_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ONexvOO8ey-"
      },
      "source": [
        "Quantized Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NF3t2CC77PDb"
      },
      "outputs": [],
      "source": [
        "class AutoencoderQuantized(nn.Module):\n",
        "    def __init__(self, quantization_levels=64):\n",
        "        super(AutoencoderQuantized, self).__init__()\n",
        "        self.quantization_levels = quantization_levels  # Burada tanımla\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "\n",
        "        scale = self.quantization_levels - 1\n",
        "        quantized_latent = torch.round(latent * scale) / scale\n",
        "\n",
        "        reconstructed = self.decoder(quantized_latent)\n",
        "        return reconstructed, quantized_latent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2OLr4F7va2"
      },
      "source": [
        "Improved Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SWUdh95s7vUK"
      },
      "outputs": [],
      "source": [
        "class ImprovedAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1),  # 64x H/2 x W/2\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # 128x H/4 x W/4\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 256x H/8 x W/8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        reconstructed = self.decoder(latent)\n",
        "        return reconstructed, latent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC-q6IZl3-gN"
      },
      "source": [
        "Model, Loss & Optimizer Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NCPl0X5F3-Ep"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Autoencoder().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAj5ZstTIY8R"
      },
      "source": [
        "VGG and perceptual Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PmbZcLYBIY2R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self, layers=['3', '8', '15', '22']):\n",
        "        super().__init__()\n",
        "        vgg_pretrained = models.vgg16(pretrained=True).features\n",
        "        self.selected_layers = layers\n",
        "        self.layers = nn.ModuleList([vgg_pretrained[i] for i in range(max(map(int, layers)) + 1)])\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if str(i) in self.selected_layers:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "def perceptual_loss(output, target, feature_extractor, criterion=nn.L1Loss()):\n",
        "    def normalize_for_vgg(x):\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1, 3, 1, 1)\n",
        "        return (x - mean) / std\n",
        "\n",
        "    output = normalize_for_vgg(output)\n",
        "    target = normalize_for_vgg(target)\n",
        "\n",
        "    output_features = feature_extractor(output)\n",
        "    target_features = feature_extractor(target)\n",
        "    loss = 0\n",
        "    for of, tf in zip(output_features, target_features):\n",
        "        loss += criterion(of, tf)\n",
        "    return loss\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def perc_train_model(model, dataloader, optimizer, num_epochs, device):\n",
        "    mse_criterion = nn.MSELoss()\n",
        "    vgg_extractor = VGGFeatureExtractor().to(device)\n",
        "    vgg_extractor.eval()  # Sadece feature extraction, eğitim yok\n",
        "\n",
        "    alpha = 0.85  # MSE loss ağırlığı\n",
        "    beta = 0.15   # Perceptual loss ağırlığı\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_recon_loss = 0.0\n",
        "        running_perceptual_loss = 0.0\n",
        "\n",
        "        for inputs, _ in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = inputs.clone()\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "\n",
        "            # MSE loss\n",
        "            recon_loss = mse_criterion(outputs, targets)\n",
        "\n",
        "            # Perceptual loss\n",
        "            p_loss = perceptual_loss(outputs, targets, vgg_extractor)\n",
        "\n",
        "            # Toplam loss\n",
        "            loss = alpha * recon_loss + beta * p_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_recon_loss += recon_loss.item() * inputs.size(0)\n",
        "            running_perceptual_loss += p_loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        epoch_recon_loss = running_recon_loss / len(dataloader.dataset)\n",
        "        epoch_p_loss = running_perceptual_loss / len(dataloader.dataset)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Total Loss: {epoch_loss:.4f}, Recon Loss: {epoch_recon_loss:.4f}, Perceptual Loss: {epoch_p_loss:.4f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reconstruct_and_visualize(model, dataloader, device, num_images=5, title=\"Reconstruction\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    images_shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in dataloader:\n",
        "            if isinstance(inputs, (list, tuple)) and len(inputs) == 2:\n",
        "                inputs = inputs[0]\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "\n",
        "            inputs = inputs.cpu()\n",
        "            outputs = outputs.cpu()\n",
        "\n",
        "            for i in range(min(num_images, inputs.size(0))):\n",
        "                input_img = inputs[i].permute(1, 2, 0).clamp(0, 1)\n",
        "                output_img = outputs[i].permute(1, 2, 0).clamp(0, 1)\n",
        "\n",
        "                fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "                axes[0].imshow(input_img.numpy())\n",
        "                axes[0].set_title(\"Original\")\n",
        "                axes[0].axis(\"off\")\n",
        "\n",
        "                axes[1].imshow(output_img.numpy())\n",
        "                axes[1].set_title(\"Reconstructed\")\n",
        "                axes[1].axis(\"off\")\n",
        "\n",
        "                fig.suptitle(title)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                images_shown += 1\n",
        "                if images_shown >= num_images:\n",
        "                    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNg8RYKL4Fsh"
      },
      "source": [
        "Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zyeEEl_04FNB"
      },
      "outputs": [],
      "source": [
        "def train_model(model, loss_fn, optimizer, train_loader, num_epochs=10, loss_name=\"Loss\", save_path=\"autoencoder.pth\"):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        epoch_compression = 0.0\n",
        "        total_batches = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            imgs, _ = data\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            # Modelden hem çıktı hem de latent temsil alınmalı\n",
        "            outputs, latent = model(imgs)\n",
        "            loss = loss_fn(outputs, imgs)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Compression ratio hesapla\n",
        "            cr = compression_ratio(imgs, latent, quantization_levels=16)\n",
        "            epoch_compression += cr\n",
        "\n",
        "        avg_loss = epoch_loss / total_batches\n",
        "        avg_compression = epoch_compression / total_batches\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], {loss_name}: {avg_loss:.4f}, Compression Ratio: {avg_compression:.2f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(\"-----------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGt04FwR72gu"
      },
      "source": [
        "Test & Visualize Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3gJqohjO4Jc6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def test_and_visualize_model(model, weight_path, test_loader, title=\"\", device='cuda',\n",
        "                             max_batches=5, max_images_per_batch=4, output_color_space=\"rgb\"):\n",
        "    \"\"\"\n",
        "    Modeli test eder, batch batch çıktı üretir ve görselleştirir.\n",
        "    output_color_space parametresi ile çıktı rgb veya yuv olabilir.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Model nesnesi\n",
        "        weight_path (str): Model ağırlıklarının yolu\n",
        "        test_loader (DataLoader): Test veri yükleyicisi\n",
        "        title (str): Görselleştirme başlığı\n",
        "        device (str): 'cuda' ya da 'cpu'\n",
        "        max_batches (int): En fazla kaç batch gösterilecek (default 5)\n",
        "        max_images_per_batch (int): Her batch'ten kaç resim gösterilecek (default 4)\n",
        "        output_color_space (str): 'rgb' veya 'yuv' (default 'rgb')\n",
        "    \"\"\"\n",
        "    model.load_state_dict(torch.load(weight_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, _) in enumerate(test_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            outputs, _ = model(imgs)\n",
        "\n",
        "            # Eğer çıktı YCbCr ise, RGB'ye çevir\n",
        "            if output_color_space.lower() == \"ycbcr\":\n",
        "                outputs = ycbcr_to_rgb(outputs)\n",
        "\n",
        "            batch_size = imgs.size(0)\n",
        "            n = min(batch_size, max_images_per_batch)\n",
        "\n",
        "            fig, axes = plt.subplots(2, n, figsize=(3 * n, 6))\n",
        "            if n == 1:\n",
        "                axes = axes.reshape(2, 1)  # axes'leri 2D array gibi kullanmak için\n",
        "\n",
        "            for i in range(n):\n",
        "                # Orijinal\n",
        "                axes[0, i].imshow(imgs[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
        "                axes[0, i].set_title(\"Original\")\n",
        "                axes[0, i].axis('off')\n",
        "\n",
        "                # Rekonstrüksiyon\n",
        "                axes[1, i].imshow(outputs[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
        "                axes[1, i].set_title(\"Reconstructed\")\n",
        "                axes[1, i].axis('off')\n",
        "\n",
        "            plt.suptitle(f\"{title} - Batch {batch_idx + 1}\")\n",
        "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "            plt.show()\n",
        "\n",
        "            if batch_idx + 1 >= max_batches:\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdTlcRj4xkIl"
      },
      "source": [
        "JPEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "C53l9WHUt2Xx"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "def jpeg_compress(img_tensor, quality=25):\n",
        "    img_pil = TF.to_pil_image(img_tensor.cpu())\n",
        "    buffer = io.BytesIO()\n",
        "    img_pil.save(buffer, format=\"JPEG\", quality=quality)\n",
        "    buffer.seek(0)\n",
        "    compressed = Image.open(buffer)\n",
        "    return TF.to_tensor(compressed).to(device)\n",
        "\n",
        "def test_jpeg_quality(test_loader, quality=25):\n",
        "    mse_metric = MeanSquaredError().to(device)\n",
        "    psnr_metric = PeakSignalNoiseRatio().to(device)\n",
        "    ssim_metric = StructuralSimilarityIndexMeasure().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            imgs, _ = data\n",
        "            imgs = imgs.to(device)\n",
        "            compressed_imgs = torch.stack([jpeg_compress(im, quality) for im in imgs])\n",
        "\n",
        "            mse_val = mse_metric(compressed_imgs, imgs).item()\n",
        "            psnr_val = psnr_metric(compressed_imgs, imgs).item()\n",
        "            ssim_val = ssim_metric(compressed_imgs, imgs).item()\n",
        "\n",
        "            print(f\"JPEG Quality {quality} - MSE: {mse_val:.6f}, PSNR: {psnr_val:.2f}, SSIM: {ssim_val:.4f}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnIdUZU7xl6P"
      },
      "source": [
        "Datasets Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "s-iqMUUhxllQ"
      },
      "outputs": [],
      "source": [
        "datasets_info = [\n",
        "    {\n",
        "        \"name\": \"Kodak\",\n",
        "        \"train_loader\": kodak_loader,\n",
        "        \"test_loader\": kodak_loader,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjtLx71L9_rq"
      },
      "source": [
        "Training with Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "collapsed": true,
        "id": "nEGfyNPi9-3L",
        "outputId": "fd648ce7-a192-41e5-b284-010f0a1ce40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Autoencoder with MSE Loss...\n",
            "Epoch [1/10], MSE Loss: 0.0523, Compression Ratio: 12.00\n",
            "Epoch [2/10], MSE Loss: 0.0514, Compression Ratio: 12.00\n",
            "Epoch [3/10], MSE Loss: 0.0501, Compression Ratio: 12.00\n",
            "Epoch [4/10], MSE Loss: 0.0475, Compression Ratio: 12.00\n",
            "Epoch [5/10], MSE Loss: 0.0433, Compression Ratio: 12.00\n",
            "Epoch [6/10], MSE Loss: 0.0353, Compression Ratio: 12.00\n",
            "Epoch [7/10], MSE Loss: 0.0252, Compression Ratio: 12.00\n",
            "Epoch [8/10], MSE Loss: 0.0177, Compression Ratio: 12.00\n",
            "Epoch [9/10], MSE Loss: 0.0142, Compression Ratio: 12.00\n",
            "Epoch [10/10], MSE Loss: 0.0121, Compression Ratio: 12.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 25 - MSE: 0.000930, PSNR: 30.31, SSIM: 0.8590\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start Autoencoder\n",
        "    model = Autoencoder().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Eğitim: MSE Loss\n",
        "    print(f\"Training {ds['name']} Autoencoder with MSE Loss...\")\n",
        "    model_path = f\"{ds['name'].lower()}_autoencoder_mse.pth\"\n",
        "    train_model(\n",
        "        model=model,\n",
        "        loss_fn=mse_loss,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        num_epochs=10,\n",
        "        loss_name=\"MSE Loss\",\n",
        "        save_path=model_path\n",
        "    )\n",
        "\n",
        "    print(f\"Testing {ds['name']} Autoencoder...\")\n",
        "    # test_and_visualize_model(model, weight_path=model_path, test_loader=test_loader, title=f\"{ds['name']} Reconstructed (MSE)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "imwW0pMxUtS4",
        "outputId": "1634a953-e9bc-4c33-b8cf-d17861adedf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Autoencoder with PSNR Loss...\n",
            "Epoch [1/10], PSNR Loss: 0.7500, Compression Ratio: 12.00\n",
            "Epoch [2/10], PSNR Loss: 0.7483, Compression Ratio: 12.00\n",
            "Epoch [3/10], PSNR Loss: 0.7453, Compression Ratio: 12.00\n",
            "Epoch [4/10], PSNR Loss: 0.7413, Compression Ratio: 12.00\n",
            "Epoch [5/10], PSNR Loss: 0.7339, Compression Ratio: 12.00\n",
            "Epoch [6/10], PSNR Loss: 0.7255, Compression Ratio: 12.00\n",
            "Epoch [7/10], PSNR Loss: 0.7085, Compression Ratio: 12.00\n",
            "Epoch [8/10], PSNR Loss: 0.6812, Compression Ratio: 12.00\n",
            "Epoch [9/10], PSNR Loss: 0.6481, Compression Ratio: 12.00\n",
            "Epoch [10/10], PSNR Loss: 0.6352, Compression Ratio: 12.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 25 - MSE: 0.001022, PSNR: 29.91, SSIM: 0.8375\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "\n",
        "    # Start Autoencoder\n",
        "    model = Autoencoder().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: PSNR Loss\n",
        "\n",
        "    print(f\"Training {ds['name']} Autoencoder with PSNR Loss...\")\n",
        "    train_model(model, psnr_loss, optimizer, train_loader, num_epochs=10, loss_name=\"PSNR Loss\", save_path=\"autoencoder_psnr.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Autoencoder...\")\n",
        "    #test_and_visualize_model(model, weight_path=\"autoencoder_psnr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (PSNR)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "PyC6CNnwuVUg",
        "outputId": "c4d0220c-2cd9-439a-b97c-57561c71e706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Autoencoder with SSIM Loss...\n",
            "Epoch [1/10], SSIM Loss: 0.5796, Compression Ratio: 12.00\n",
            "Epoch [2/10], SSIM Loss: 0.5772, Compression Ratio: 12.00\n",
            "Epoch [3/10], SSIM Loss: 0.5768, Compression Ratio: 12.00\n",
            "Epoch [4/10], SSIM Loss: 0.5764, Compression Ratio: 12.00\n",
            "Epoch [5/10], SSIM Loss: 0.5755, Compression Ratio: 12.00\n",
            "Epoch [6/10], SSIM Loss: 0.5734, Compression Ratio: 12.00\n",
            "Epoch [7/10], SSIM Loss: 0.5686, Compression Ratio: 12.00\n",
            "Epoch [8/10], SSIM Loss: 0.5579, Compression Ratio: 12.00\n",
            "Epoch [9/10], SSIM Loss: 0.5367, Compression Ratio: 12.00\n",
            "Epoch [10/10], SSIM Loss: 0.5037, Compression Ratio: 12.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 25 - MSE: 0.000959, PSNR: 30.18, SSIM: 0.8639\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "\n",
        "    # Start Autoencoder\n",
        "    model = Autoencoder().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training:  SSIM Loss\n",
        "    print(f\"Training {ds['name']} Autoencoder with SSIM Loss...\")\n",
        "    train_model(model, ssim_loss, optimizer, train_loader, num_epochs=10, loss_name=\"SSIM Loss\", save_path=\"autoencoder_ssim.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Autoencoder...\")\n",
        "    #test_and_visualize_model(model, weight_path=\"autoencoder_ssim.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (SSIM)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=25)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnLrBNkt-C_H"
      },
      "source": [
        "Training with Quantized Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "mCgZ4Hvpuc3Y",
        "outputId": "9021cced-e187-436f-f707-7a25b681fad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Quantized Autoencoder with MSE Loss...\n",
            "Epoch [1/10], Quantized MSE Loss: 0.0523, Compression Ratio: 12.00\n",
            "Epoch [2/10], Quantized MSE Loss: 0.0516, Compression Ratio: 12.00\n",
            "Epoch [3/10], Quantized MSE Loss: 0.0510, Compression Ratio: 12.00\n",
            "Epoch [4/10], Quantized MSE Loss: 0.0502, Compression Ratio: 12.00\n",
            "Epoch [5/10], Quantized MSE Loss: 0.0493, Compression Ratio: 12.00\n",
            "Epoch [6/10], Quantized MSE Loss: 0.0483, Compression Ratio: 12.00\n",
            "Epoch [7/10], Quantized MSE Loss: 0.0471, Compression Ratio: 12.00\n",
            "Epoch [8/10], Quantized MSE Loss: 0.0459, Compression Ratio: 12.00\n",
            "Epoch [9/10], Quantized MSE Loss: 0.0444, Compression Ratio: 12.00\n",
            "Epoch [10/10], Quantized MSE Loss: 0.0430, Compression Ratio: 12.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Quantized Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.007369, PSNR: 21.33, SSIM: 0.5339\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start AutoencoderQuantized\n",
        "    model1 = AutoencoderQuantized(quantization_levels=16).to(device)\n",
        "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: MSE Loss\n",
        "    print(f\"Training {ds['name']} Quantized Autoencoder with MSE Loss...\")\n",
        "    train_model(model1, mse_loss, optimizer1, train_loader, num_epochs=10, loss_name=\"Quantized MSE Loss\", save_path=\"autoencoder_mse_quantized.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Quantized Autoencoder...\")\n",
        "    #test_and_visualize_model(model1, weight_path=\"autoencoder_mse_quantized.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (MSE + Quant)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "N02dYYV4ucxc",
        "outputId": "c53eda8d-033d-443a-e62a-8adcc55d4280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Quantized Autoencoder with PSNR Loss...\n",
            "Epoch [1/10], Quantized PSNR Loss: 0.7432, Compression Ratio: 12.00\n",
            "Epoch [2/10], Quantized PSNR Loss: 0.7409, Compression Ratio: 12.00\n",
            "Epoch [3/10], Quantized PSNR Loss: 0.7407, Compression Ratio: 12.00\n",
            "Epoch [4/10], Quantized PSNR Loss: 0.7398, Compression Ratio: 12.00\n",
            "Epoch [5/10], Quantized PSNR Loss: 0.7400, Compression Ratio: 12.00\n",
            "Epoch [6/10], Quantized PSNR Loss: 0.7377, Compression Ratio: 12.00\n",
            "Epoch [7/10], Quantized PSNR Loss: 0.7364, Compression Ratio: 12.00\n",
            "Epoch [8/10], Quantized PSNR Loss: 0.7338, Compression Ratio: 12.00\n",
            "Epoch [9/10], Quantized PSNR Loss: 0.7305, Compression Ratio: 12.00\n",
            "Epoch [10/10], Quantized PSNR Loss: 0.7251, Compression Ratio: 12.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Quantized Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.007482, PSNR: 21.26, SSIM: 0.5495\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start AutoencoderQuantized\n",
        "    model1 = AutoencoderQuantized(quantization_levels=16).to(device)\n",
        "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: PSNR Loss\n",
        "    print(f\"Training {ds['name']} Quantized Autoencoder with PSNR Loss...\")\n",
        "    train_model(model1, psnr_loss, optimizer1, train_loader, num_epochs=10, loss_name=\"Quantized PSNR Loss\", save_path=\"autoencoder_psnr_quantized.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Quantized Autoencoder...\")\n",
        "    #test_and_visualize_model(model1, weight_path=\"autoencoder_psnr_quantized.pth\", test_loader=test_loader, title= f\"{ds['name']}Reconstructed (PSNR + Quant)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "wu6hFFQ5-CrT",
        "outputId": "04adf84e-698f-48ac-b085-d3f4018f6b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Quantized Autoencoder with SSIM Loss...\n",
            "Epoch [1/10], Quantized SSIM Loss: 0.5782, Compression Ratio: 12.00\n",
            "Epoch [2/10], Quantized SSIM Loss: 0.5762, Compression Ratio: 12.00\n",
            "Epoch [3/10], Quantized SSIM Loss: 0.5754, Compression Ratio: 12.00\n",
            "Epoch [4/10], Quantized SSIM Loss: 0.5748, Compression Ratio: 12.00\n",
            "Epoch [5/10], Quantized SSIM Loss: 0.5738, Compression Ratio: 12.00\n",
            "Epoch [6/10], Quantized SSIM Loss: 0.5725, Compression Ratio: 12.00\n",
            "Epoch [7/10], Quantized SSIM Loss: 0.5709, Compression Ratio: 12.00\n",
            "Epoch [8/10], Quantized SSIM Loss: 0.5685, Compression Ratio: 12.00\n",
            "Epoch [9/10], Quantized SSIM Loss: 0.5652, Compression Ratio: 12.00\n",
            "Epoch [10/10], Quantized SSIM Loss: 0.5607, Compression Ratio: 12.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Quantized Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.006751, PSNR: 21.71, SSIM: 0.5210\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start AutoencoderQuantized\n",
        "    model1 = AutoencoderQuantized(quantization_levels=16).to(device)\n",
        "    optimizer1 = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: SSIM Loss\n",
        "    print(f\"Training {ds['name']} Quantized Autoencoder with SSIM Loss...\")\n",
        "    train_model(model1, ssim_loss, optimizer1, train_loader, num_epochs=10, loss_name=\"Quantized SSIM Loss\", save_path=\"autoencoder_ssim_quantized.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Quantized Autoencoder...\")\n",
        "    #test_and_visualize_model(model1, weight_path=\"autoencoder_ssim_quantized.pth\", test_loader=test_loader, title= f\"{ds['name']}Reconstructed (SSIM + Quant)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gekkrz9A8eq1"
      },
      "source": [
        "Training with Improved Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "2SAy2rXI8ekM",
        "outputId": "3db20344-88c6-41fb-f4ea-2233ee8a84db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Improved Autoencoder with MSE Loss...\n",
            "Epoch [1/10], Improved MSE Loss: 0.0407, Compression Ratio: 6.00\n",
            "Epoch [2/10], Improved MSE Loss: 0.0193, Compression Ratio: 6.00\n",
            "Epoch [3/10], Improved MSE Loss: 0.0148, Compression Ratio: 6.00\n",
            "Epoch [4/10], Improved MSE Loss: 0.0151, Compression Ratio: 6.00\n",
            "Epoch [5/10], Improved MSE Loss: 0.0128, Compression Ratio: 6.00\n",
            "Epoch [6/10], Improved MSE Loss: 0.0122, Compression Ratio: 6.00\n",
            "Epoch [7/10], Improved MSE Loss: 0.0126, Compression Ratio: 6.00\n",
            "Epoch [8/10], Improved MSE Loss: 0.0117, Compression Ratio: 6.00\n",
            "Epoch [9/10], Improved MSE Loss: 0.0098, Compression Ratio: 6.00\n",
            "Epoch [10/10], Improved MSE Loss: 0.0086, Compression Ratio: 6.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Improved Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.007161, PSNR: 21.45, SSIM: 0.5789\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start AutoencoderQuantized\n",
        "    model2 = ImprovedAutoencoder().to(device)\n",
        "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: MSE Loss\n",
        "    print(f\"Training {ds['name']} Improved Autoencoder with MSE Loss...\")\n",
        "    train_model(model2, mse_loss, optimizer2, train_loader, num_epochs=10, loss_name=\"Improved MSE Loss\", save_path=\"autoencoder_mse_impr.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Improved Autoencoder...\")\n",
        "    #test_and_visualize_model(model2, weight_path=\"autoencoder_mse_impr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (MSE + Impr)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "XkbYvVTT82wQ",
        "outputId": "78740fcf-4157-4d79-b779-c54ab2755aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Improved Autoencoder with PSNR Loss...\n",
            "Epoch [1/10], Improved PSNR Loss: 0.7091, Compression Ratio: 6.00\n",
            "Epoch [2/10], Improved PSNR Loss: 0.6369, Compression Ratio: 6.00\n",
            "Epoch [3/10], Improved PSNR Loss: 0.6234, Compression Ratio: 6.00\n",
            "Epoch [4/10], Improved PSNR Loss: 0.6087, Compression Ratio: 6.00\n",
            "Epoch [5/10], Improved PSNR Loss: 0.6110, Compression Ratio: 6.00\n",
            "Epoch [6/10], Improved PSNR Loss: 0.6081, Compression Ratio: 6.00\n",
            "Epoch [7/10], Improved PSNR Loss: 0.5855, Compression Ratio: 6.00\n",
            "Epoch [8/10], Improved PSNR Loss: 0.5992, Compression Ratio: 6.00\n",
            "Epoch [9/10], Improved PSNR Loss: 0.5875, Compression Ratio: 6.00\n",
            "Epoch [10/10], Improved PSNR Loss: 0.5739, Compression Ratio: 6.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Improved Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.006410, PSNR: 21.93, SSIM: 0.5365\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start ImprovedAutoencoder\n",
        "    model2 = ImprovedAutoencoder().to(device)\n",
        "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: PSNR Loss\n",
        "    print(f\"Training {ds['name']} Improved Autoencoder with PSNR Loss...\")\n",
        "    train_model(model2, psnr_loss, optimizer2, train_loader, num_epochs=10, loss_name=\"Improved PSNR Loss\", save_path=\"autoencoder_psnr_impr.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Improved Autoencoder...\")\n",
        "    #test_and_visualize_model(model2, weight_path=\"autoencoder_psnr_impr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (PSNR + Impr)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "pdN42rZk8-yg",
        "outputId": "2d9d0e65-2e0e-465d-a237-d9b8d6cbbdbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Improved Autoencoder with SSIM Loss...\n",
            "Epoch [1/10], Improved SSIM Loss: 0.7956, Compression Ratio: 6.00\n",
            "Epoch [2/10], Improved SSIM Loss: 0.6478, Compression Ratio: 6.00\n",
            "Epoch [3/10], Improved SSIM Loss: 0.6135, Compression Ratio: 6.00\n",
            "Epoch [4/10], Improved SSIM Loss: 0.5917, Compression Ratio: 6.00\n",
            "Epoch [5/10], Improved SSIM Loss: 0.5839, Compression Ratio: 6.00\n",
            "Epoch [6/10], Improved SSIM Loss: 0.5786, Compression Ratio: 6.00\n",
            "Epoch [7/10], Improved SSIM Loss: 0.5716, Compression Ratio: 6.00\n",
            "Epoch [8/10], Improved SSIM Loss: 0.5726, Compression Ratio: 6.00\n",
            "Epoch [9/10], Improved SSIM Loss: 0.5716, Compression Ratio: 6.00\n",
            "Epoch [10/10], Improved SSIM Loss: 0.5657, Compression Ratio: 6.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Improved Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.006263, PSNR: 22.03, SSIM: 0.5635\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start ImprovedAutoencoder\n",
        "    model2 = ImprovedAutoencoder().to(device)\n",
        "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: SSIM Loss\n",
        "    print(f\"Training {ds['name']} Improved Autoencoder with SSIM Loss...\")\n",
        "    train_model(model2, ssim_loss, optimizer2, train_loader, num_epochs=10, loss_name=\"Improved SSIM Loss\", save_path=\"autoencoder_ssim_impr.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Improved Autoencoder...\")\n",
        "    #test_and_visualize_model(model2, weight_path=\"autoencoder_ssim_impr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (SSIM + Impr)\", output_color_space=\"rgb\")\n",
        "\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "te4bFbDF9Lj4",
        "outputId": "ea0a5765-cefc-47d4-f3e3-0d7db30630ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Dataset: Kodak ---\n",
            "Training Kodak Improved Autoencoder with Combined SSIM Loss...\n",
            "Epoch [1/10], Improved Combined SSIM Loss: 0.4267, Compression Ratio: 6.00\n",
            "Epoch [2/10], Improved Combined SSIM Loss: 0.3333, Compression Ratio: 6.00\n",
            "Epoch [3/10], Improved Combined SSIM Loss: 0.3286, Compression Ratio: 6.00\n",
            "Epoch [4/10], Improved Combined SSIM Loss: 0.3223, Compression Ratio: 6.00\n",
            "Epoch [5/10], Improved Combined SSIM Loss: 0.3204, Compression Ratio: 6.00\n",
            "Epoch [6/10], Improved Combined SSIM Loss: 0.3207, Compression Ratio: 6.00\n",
            "Epoch [7/10], Improved Combined SSIM Loss: 0.3171, Compression Ratio: 6.00\n",
            "Epoch [8/10], Improved Combined SSIM Loss: 0.3165, Compression Ratio: 6.00\n",
            "Epoch [9/10], Improved Combined SSIM Loss: 0.3134, Compression Ratio: 6.00\n",
            "Epoch [10/10], Improved Combined SSIM Loss: 0.3106, Compression Ratio: 6.00\n",
            "-----------------------------------------------\n",
            "Testing Kodak Improved Autoencoder...\n",
            "JPEG compression comparison for Kodak dataset:\n",
            "JPEG Quality 2 - MSE: 0.007454, PSNR: 21.28, SSIM: 0.5083\n"
          ]
        }
      ],
      "source": [
        "for ds in datasets_info:\n",
        "    print(f\"\\n--- Dataset: {ds['name']} ---\")\n",
        "    train_loader = ds[\"train_loader\"]\n",
        "    test_loader = ds[\"test_loader\"]\n",
        "\n",
        "    # Start ImprovedAutoencoder\n",
        "    model2 = ImprovedAutoencoder().to(device)\n",
        "    optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "    # Training: Combined Loss\n",
        "    print(f\"Training {ds['name']} Improved Autoencoder with Combined SSIM Loss...\")\n",
        "    train_model(model2, combined_loss, optimizer2, train_loader, num_epochs=10, loss_name=\"Improved Combined SSIM Loss\", save_path=\"autoencoder_mse_ssim_impr.pth\")\n",
        "\n",
        "    print(f\"Testing {ds['name']} Improved Autoencoder...\")\n",
        "    #test_and_visualize_model(model2, weight_path=\"autoencoder_mse_ssim_impr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (MSE + SSIM + Impr)\", output_color_space=\"rgb\")\n",
        "\n",
        "    print(f\"JPEG compression comparison for {ds['name']} dataset:\")\n",
        "    test_jpeg_quality(test_loader, quality=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii6zDyOgJAc_",
        "outputId": "c2176576-1dea-4b9a-e8b5-f923152ad075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Improved Autoencoder with Perceptual Loss...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 9824, 8528, 17424, 6164) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
            "\u001b[31mEmpty\u001b[39m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m optimizer2 = torch.optim.Adam(model2.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining Improved Autoencoder with Perceptual Loss...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mperc_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkodak_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mperc_train_model\u001b[39m\u001b[34m(model, dataloader, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m     56\u001b[39m running_recon_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     57\u001b[39m running_perceptual_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1297\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1296\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1298\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1299\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 9824, 8528, 17424, 6164) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "model2 = ImprovedAutoencoder().to(device)\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Training Improved Autoencoder with Perceptual Loss...\")\n",
        "perc_train_model(model2, kodak_loader1, optimizer2, num_epochs=30, device=\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "8eOYLJocLqs4",
        "outputId": "38cb536a-6bf1-4247-c81b-2d1add719581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Improved Autoencoder...\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting Improved Autoencoder...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#test_and_visualize_model(model2, weight_path=\"autoencoder_perceptual_impr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (Perceptual Loss)\", output_color_space=\"rgb\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mreconstruct_and_visualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkodak_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReconstruction VGG/PL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mJPEG compression comparison for dataset:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m test_jpeg_quality(test_loader, quality=\u001b[32m2\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mreconstruct_and_visualize\u001b[39m\u001b[34m(model, dataloader, device, num_images, title)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreconstruct_and_visualize\u001b[39m(model, dataloader, device, num_images=\u001b[32m5\u001b[39m, title=\u001b[33m\"\u001b[39m\u001b[33mReconstruction\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     92\u001b[39m     model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     images_shown = \u001b[32m0\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
            "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "print(\"Testing Improved Autoencoder...\")\n",
        "#test_and_visualize_model(model2, weight_path=\"autoencoder_perceptual_impr.pth\", test_loader=test_loader, title=f\"{ds['name']} Reconstructed (Perceptual Loss)\", output_color_space=\"rgb\")\n",
        "reconstruct_and_visualize(model2, kodak_loader1, device=\"cuda\", num_images=24, title=\"Reconstruction VGG/PL\")\n",
        "\n",
        "print(\"JPEG compression comparison for dataset:\")\n",
        "test_jpeg_quality(test_loader, quality=2)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
